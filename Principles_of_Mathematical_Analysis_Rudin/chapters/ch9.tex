% --- chapters/chapter9.tex ---
% Generated by Gemini (Google AI) on 2025-04-07.
% Contains extracted items from W. Rudin, PMA, Chapter 9.
% Assumes \rank and \id macros are defined in macros.tex

\chapter{Functions of Several Variables}
\label{chap:rudin9}

\section{Linear Transformations}

We begin this chapter with a discussion of sets of vectors in
euclidean n-space $\R^n$. The algebraic facts presented here extend
without change to finite-dimensional vector spaces over any field of
scalars. However, for our purposes it is quite sufficient to stay
within the familiar framework provided by the euclidean spaces.

\begin{definition}[Vector Space Definitions]
  \label{def:chap9:vector_space_definitions}
  ~ % Using enumerate for parts (a) through (e)
  \begin{enumerate}
    \item[(a)] A nonempty set $X \subset \R^n$ is a \textbf{vector
      space} if $\vect{x} + \vect{y} \in X$ and $c\vect{x} \in X$ for
      all $\vect{x} \in X, \vect{y} \in X,$ and for all scalars c.

    \item[(b)] If $\vect{x}_1, \dots, \vect{x}_k \in \R^n$ and $c_1,
      \dots, c_k$ are scalars, the vector
      \[
        c_1 \vect{x}_1 + \dots + c_k \vect{x}_k
      \]
      is called a \textbf{linear combination} of $\vect{x}_1, \dots,
      \vect{x}_k$. If $S \subset \R^n$ and if $E$ is the set of all
      linear combinations of elements of S, we say that S
      \textbf{spans} $E$, or that E is the \textbf{span} of S.

      Observe that every span is a vector space.

    \item[(c)] A set consisting of vectors $\vect{x}_1, \dots,
      \vect{x}_k$ (we shall use the notation $\{ \vect{x}_1, \dots,
      \vect{x}_k \}$ for such a set) is said to be
      \textbf{independent} if the relation
      \[
        c_1 \vect{x}_1 + \dots + c_k \vect{x}_k = \vect{0}
      \]
      implies that $c_1 = \dots = c_k = 0$. Otherwise $\{ \vect{x}_1,
      \dots, \vect{x}_k \}$ is said to be \textbf{dependent}.

      Observe that no independent set contains the null vector.

    \item[(d)] If a vector space X contains an independent set of r
      vectors but contains no independent set of $r+1$ vectors, we
      say that X has \textbf{dimension} r, and write: $\dim X = r$.

      The set consisting of $\vect{0}$ alone is a vector space; its
      dimension is 0.

    \item[(e)] An independent subset of a vector space X which spans
      X is called a \textbf{basis} of X.

      Observe that if $B = \{ \vect{x}_1, \dots, \vect{x}_r \}$ is a
      basis of X, then every $\vect{x} \in X$ has a unique
      representation of the form $\vect{x} = \sum c_j \vect{x}_j$.
      Such a representation exists since B spans X, and it is unique
      since B is independent. The numbers $c_1, \dots, c_r$ are
      called the \textbf{coordinates} of $\vect{x}$ with respect to the basis B.

      The most familiar example of a basis is the set $\{ \vect{e}_1,
      \dots, \vect{e}_n \}$ where $\vect{e}_j$ is the vector in
      $\R^n$ whose jth coordinate is 1 and whose other coordinates
      are all 0. If $\vect{x} \in \R^n, \vect{x} = (x_1, \dots,
      x_n),$ then $\vect{x} = \sum x_j \vect{e}_j$. We shall call $\{
      \vect{e}_1, \dots, \vect{e}_n \}$ the \textbf{standard basis} of $\R^n$.
  \end{enumerate}
\end{definition}

% --- End of transcription chunk ---

% --- Previous content from chapters/chapter9.tex above ---

\begin{theorem}
  \label{thm:chap9:span_implies_dim_lt_r}
  Let r be a positive integer. If a vector space X is spanned by a
  set of r vectors, then $\dim X < r$.
\end{theorem}
% Proof omitted as requested.

\begin{corollary}
  \label{cor:chap9:dim_Rn_is_n}
  $\dim \R^n = n$.
\end{corollary}
% Proof omitted as requested.

% --- End of transcription chunk ---

% --- Should be placed after the Corollary to Thm 9.2 ---

\begin{theorem}
  \label{thm:chap9:basis_properties}
  Suppose X is a vector space, and $\dim X = n$.
  \begin{enumerate}
    \item[(a)] A set E of n vectors in X spans X if and only if E is
      independent.
    \item[(b)] X has a basis, and every basis consists of n vectors.
    \item[(c)] If $1 \le r \le n$ and $\{ \vect{y}_1, \dots,
      \vect{y}_r \}$ is an independent set in X, then X has a basis
      containing $\{ \vect{y}_1, \dots, \vect{y}_r \}$.
  \end{enumerate}
\end{theorem}
% Proof omitted as requested.

% --- End of Theorem 9.3 ---
% --- Previous content (up to Thm 9.3) from chapters/chapter9.tex above ---

\begin{definition}[Linear Transformation]
  \label{def:chap9:linear_transformation}
  A mapping $A: X \to Y$ between vector spaces X and Y is a
  \textbf{linear transformation} if $A(\vect{x}_1 + \vect{x}_2) =
  A\vect{x}_1 + A\vect{x}_2$ and $A(c\vect{x}) = cA\vect{x}$ for all
  $\vect{x}, \vect{x}_1, \vect{x}_2 \in X$ and scalars c. (Note:
  $A\vect{0} = \vect{0}$.)

  A is determined by its action on a basis $\{ \vect{x}_1, \dots,
  \vect{x}_n \}$ of X: if $\vect{x} = \sum c_i \vect{x}_i$, then
  $A\vect{x} = \sum c_i A\vect{x}_i$.

  Linear transformations $A: X \to X$ are \textbf{linear operators}.
  An operator A is \textbf{invertible} if it is one-to-one and maps X
  onto X. Its inverse $A^{-1}$ satisfies $A^{-1}(A\vect{x}) =
  A(A^{-1}\vect{x}) = \vect{x}$ and is also linear.
\end{definition}

\begin{theorem}
  \label{thm:chap9:linear_op_one_to_one_iff_onto}
  A linear operator A on a finite-dimensional vector space X is
  one-to-one if and only if the range of A is all of X.
\end{theorem}
% Proof omitted.

\begin{definition}[Space of Linear Transformations, Norm]
  \label{def:chap9:LXY_norm}
  ~ % Using enumerate for parts (a) through (c)
  \begin{enumerate}
    \item[(a)] $\mathcal{L}(X, Y)$ denotes the vector space of all
      linear transformations from X to Y. $\mathcal{L}(X) =
      \mathcal{L}(X, X)$. Addition and scalar multiplication are
      defined pointwise: $(c_1 A_1 + c_2 A_2)\vect{x} = c_1 A_1
      \vect{x} + c_2 A_2 \vect{x}$.

    \item[(b)] If $A \in \mathcal{L}(X, Y)$ and $B \in \mathcal{L}(Y,
      Z)$, their product $BA \in \mathcal{L}(X, Z)$ is the
      composition $B(A\vect{x})$.

    \item[(c)] For $A \in \mathcal{L}(\R^n, \R^m)$, the \textbf{norm}
      $\norm{A} = \sup \{ |A\vect{x}| : |\vect{x}| \le 1 \}$. The
      inequality $|A\vect{x}| \le \norm{A} |\vect{x}|$ holds for all
      $\vect{x} \in \R^n$.
  \end{enumerate}
\end{definition}

\begin{theorem}[Properties of Operator Norm]
  \label{thm:chap9:operator_norm_properties}
  ~ % Using enumerate for parts (a) through (c)
  \begin{enumerate}
    \item[(a)] If $A \in \mathcal{L}(\R^n, \R^m)$, then $\norm{A} <
      \infty$ and A is uniformly continuous.
    \item[(b)] $\mathcal{L}(\R^n, \R^m)$ is a metric space with
      distance $\norm{A-B}$. The norm satisfies $\norm{A+B} \le
      \norm{A} + \norm{B}$ and $\norm{cA} = |c|\norm{A}$.
    \item[(c)] If $A \in \mathcal{L}(\R^n, \R^m)$ and $B \in
      \mathcal{L}(\R^m, \R^k)$ then $\norm{BA} \le \norm{B} \norm{A}$.
  \end{enumerate}
\end{theorem}
% Proof omitted.

\begin{theorem}[Set of Invertible Operators]
  \label{thm:chap9:invertible_operators_open_set}
  Let $\Omega$ be the set of all invertible linear operators on $\R^n$.
  \begin{enumerate}
    \item[(a)] If $A \in \Omega$, $B \in \mathcal{L}(\R^n)$, and
      $\norm{B - A} \cdot \norm{A^{-1}} < 1$, then $B \in \Omega$.
    \item[(b)] $\Omega$ is an open subset of $\mathcal{L}(\R^n)$, and
      the mapping $A \to A^{-1}$ is continuous on $\Omega$.
  \end{enumerate}
\end{theorem}
% Proof omitted.

\begin{remark}[Matrices]
  \label{rem:chap9:matrices}
  Relative to bases $\{ \vect{x}_j \}$ for X and $\{ \vect{y}_i \}$
  for Y, any $A \in \mathcal{L}(X, Y)$ corresponds to a unique $m
  \times n$ matrix $[A] = (a_{ij})$ defined by
  \begin{equation*} %\label{eq:chap9:matrix_representation_concise} %
    % Removed label as it was not referenced
    A\vect{x}_j = \sum_{i=1}^m a_{ij} \vect{y}_i.
  \end{equation*}
  If $\vect{x} = \sum c_j \vect{x}_j$, the coordinates of $A\vect{x}$
  w.r.t. $\{ \vect{y}_i \}$ are $\sum_j a_{ij} c_j$. This
  correspondence depends on the chosen bases.

  If $A \in \mathcal{L}(X, Y)$, $B \in \mathcal{L}(Y, Z)$, the matrix
  product $[B][A]$ corresponds to the composition $BA$, with entries
  $([BA])_{kj} = \sum_i ([B])_{ki} ([A])_{ij}$.

  For $A \in \mathcal{L}(\R^n, \R^m)$ relative to standard bases,
  $\norm{A} \le (\sum_{i,j} a_{ij}^2)^{1/2}$.
  If matrix entries $a_{ij}(p)$ depend continuously on a parameter
  $p$ in a metric space S, then the map $p \to A_p$ is continuous
  into $\mathcal{L}(\R^n, \R^m)$.
\end{remark}

% --- End of transcription chunk ---

% --- Previous content (up to Rem 9.9) from chapters/chapter9.tex above ---

\section{Differentiation}

% Preliminaries (Condensed from 9.10)
The derivative $f'(x)$ of a function $f: (a,b) \subset \R \to \R^m$
at a point $x$ can be viewed as the linear transformation from $\R$
to $\R^m$ (represented by $h \mapsto f'(x)h$) that best approximates
the change $f(x+h) - f(x)$ for small $h$. This motivates the
definition for functions of several variables.

\begin{definition}[Differentiability]
  \label{def:chap9:differentiability}
  Suppose E is an open set in $\R^n$, $\map{f}{E}{\R^m}$, and
  $\vect{x} \in E$. If there exists a linear transformation $A \in
  \mathcal{L}(\R^n, \R^m)$ such that
  \begin{equation} \label{eq:chap9:diff_limit_def}
    \lim_{\vect{h} \to \vect{0}} \frac{|f(\vect{x}+\vect{h}) -
    f(\vect{x}) - A\vect{h}|}{|\vect{h}|} = 0,
  \end{equation}
  then we say that f is \textbf{differentiable} at $\vect{x}$, and we
  write $f'(\vect{x}) = A$. If f is differentiable at every $\vect{x}
  \in E$, we say f is differentiable in E.
\end{definition}

\begin{theorem}[Uniqueness of Derivative]
  \label{thm:chap9:uniqueness_derivative}
  Suppose E and f are as in Definition
  \ref{def:chap9:differentiability}, $\vect{x} \in E$, and
  \eqref{eq:chap9:diff_limit_def} holds with $A=A_1$ and with
  $A=A_2$. Then $A_1 = A_2$.
\end{theorem}
% Proof omitted.

\begin{remark}[Remarks on Differentiability]
  \label{rem:chap9:diff_remarks}
  ~ % Using enumerate
  \begin{enumerate}
    \item[(a)] The condition \eqref{eq:chap9:diff_limit_def} can be
      written $f(\vect{x}+\vect{h}) - f(\vect{x}) =
      f'(\vect{x})\vect{h} + r(\vect{h})$, where
      $|r(\vect{h})|/|\vect{h}| \to 0$ as $\vect{h} \to \vect{0}$.
      $f'(\vect{x})\vect{h}$ is the best linear approximation of the
      increment $f(\vect{x}+\vect{h}) - f(\vect{x})$.
    \item[(b)] If f is differentiable in E, then $f'$ is a function
      mapping E into $\mathcal{L}(\R^n, \R^m)$.
    \item[(c)] If f is differentiable at $\vect{x}$, then f is
      continuous at $\vect{x}$.
    \item[(d)] The derivative $f'(\vect{x})$ is often called the
      \textbf{differential} or \textbf{total derivative} of f at $\vect{x}$.
  \end{enumerate}
\end{remark}

\begin{example}[Derivative of a Linear Transformation]
  \label{ex:chap9:derivative_linear_map}
  If $A \in \mathcal{L}(\R^n, \R^m)$ and if $\vect{x} \in \R^n$, then
  $A'(\vect{x}) = A$.
  (The proof follows since $A(\vect{x}+\vect{h}) - A\vect{x} =
  A\vect{h}$, so the remainder term $r(\vect{h})$ is identically zero.)
\end{example}

\begin{theorem}[Chain Rule]
  \label{thm:chap9:chain_rule}
  Suppose E is an open set in $\R^n$, $\map{f}{E}{\R^m}$, f is
  differentiable at $\vect{x}_0 \in E$, $\map{g}{\text{open set
  containing } f(E)}{\R^k}$, and g is differentiable at
  $f(\vect{x}_0)$. Then the mapping $\map{F}{E}{\R^k}$ defined by
  $F(\vect{x}) = g(f(\vect{x}))$ is differentiable at $\vect{x}_0$, and
  \begin{equation*} %\label{eq:chap9:chain_rule_formula} % No label
    % needed unless referenced
    F'(\vect{x}_0) = g'(f(\vect{x}_0)) f'(\vect{x}_0).
  \end{equation*}
  (The right side is the product of linear transformations.)
\end{theorem}
% Proof omitted.

\begin{definition}[Partial Derivatives]
  \label{def:chap9:partial_derivatives}
  Let $\map{f}{E \subset \R^n open}{\R^m}$ with component functions
  $f_1, \dots, f_m$ such that $f(\vect{x}) = \sum_{i=1}^m
  f_i(\vect{x}) \vect{u}_i$, where $\{ \vect{u}_1, \dots, \vect{u}_m
  \}$ is the standard basis of $\R^m$. For $\vect{x} \in E$, $1 \le i
  \le m$, $1 \le j \le n$, the \textbf{partial derivative} of $f_i$
  with respect to $x_j$ at $\vect{x}$ is defined as
  \[
    (D_j f_i)(\vect{x}) = \lim_{t \to 0} \frac{f_i(\vect{x} +
    t\vect{e}_j) - f_i(\vect{x})}{t},
  \]
  provided the limit exists (where $\{ \vect{e}_1, \dots, \vect{e}_n
  \}$ is the standard basis of $\R^n$). The notation $\frac{\partial
  f_i}{\partial x_j}$ is also used for $(D_j f_i)(\vect{x})$.
\end{definition}

\begin{theorem}[Derivative and Partial Derivatives]
  \label{thm:chap9:derivative_implies_partials}
  Suppose $\map{f}{E \subset \R^n open}{\R^m}$ is differentiable at a
  point $\vect{x} \in E$. Then all partial derivatives $(D_j
  f_i)(\vect{x})$ exist ($1 \le i \le m, 1 \le j \le n$), and
  \begin{equation*} %\label{eq:chap9:fprime_ej_formula}
    f'(\vect{x})\vect{e}_j = \sum_{i=1}^m (D_j f_i)(\vect{x})
    \vect{u}_i \quad (1 \le j \le n).
  \end{equation*}
  Consequently, the matrix $[f'(\vect{x})]$ relative to the standard
  bases has $(D_j f_i)(\vect{x})$ as the entry in the i-th row and
  j-th column. If $\vect{h} = \sum h_j \vect{e}_j \in \R^n$, then
  \begin{equation*} %\label{eq:chap9:fprime_h_formula}
    f'(\vect{x})\vect{h} = \sum_{i=1}^m \left\{ \sum_{j=1}^n (D_j
    f_i)(\vect{x}) h_j \right\} \vect{u}_i.
  \end{equation*}
\end{theorem}
% Proof omitted.

% --- End of transcription chunk ---

% --- Previous content (up to Thm 9.17) from chapters/chapter9.tex above ---

\begin{example}[Gradient and Directional Derivative]
  \label{ex:chap9:gradient_directional_deriv}
  Let $\map{\gamma}{(a,b) \subset \R^1}{E \subset \R^n open}$ be a
  differentiable curve, and let $\map{f}{E}{\R}$ be a differentiable
  real-valued function. Define $g(t) = f(\gamma(t))$. The chain rule
  (\ref{thm:chap9:chain_rule}) gives $g'(t) = f'(\gamma(t))\gamma'(t)$.

  Regarding $f'(\vect{x}) \in \mathcal{L}(\R^n, \R)$ as a row matrix
  $[ (D_1 f)(\vect{x}), \dots, (D_n f)(\vect{x}) ]$ and $\gamma'(t)
  \in \mathcal{L}(\R^1, \R^n)$ as a column matrix $[\gamma'_1(t),
  \dots, \gamma'_n(t)]^T$, their product (matrix multiplication)
  yields the real number:
  \begin{equation*} %\label{eq:chap9:chain_rule_gradient_form_sum}
    g'(t) = \sum_{i=1}^n (D_i f)(\gamma(t)) \gamma'_i(t).
  \end{equation*}
  Define the \textbf{gradient} of f at $\vect{x}$ as the vector
  \begin{equation*} %\label{eq:chap9:gradient_def}
    (\nabla f)(\vect{x}) = \sum_{i=1}^n (D_i f)(\vect{x}) \vect{e}_i.
  \end{equation*}
  Since $\gamma'(t) = \sum \gamma'_i(t) \vect{e}_i$, the chain rule
  can be written using the scalar product:
  \begin{equation*} %\label{eq:chap9:chain_rule_gradient_form_dot}
    g'(t) = (\nabla f)(\gamma(t)) \cdot \gamma'(t).
  \end{equation*}
  If we take a specific curve $\gamma(t) = \vect{x} + t\vect{u}$
  where $\vect{u} \in \R^n$ is a unit vector ($|\vect{u}|=1$), then
  $\gamma'(t) = \vect{u}$. The derivative $g'(0)$ is called the
  \textbf{directional derivative} of f at $\vect{x}$ in the direction
  $\vect{u}$, denoted $(D_{\vect{u}}f)(\vect{x})$. It satisfies:
  \begin{equation*} %\label{eq:chap9:directional_derivative_limit}
    (D_{\vect{u}}f)(\vect{x}) = \lim_{t \to 0} \frac{f(\vect{x} +
    t\vect{u}) - f(\vect{x})}{t} = (\nabla f)(\vect{x}) \cdot \vect{u}.
  \end{equation*}
  If $\vect{u} = \sum u_i \vect{e}_i$, then
  $(D_{\vect{u}}f)(\vect{x}) = \sum_{i=1}^n (D_i f)(\vect{x}) u_i$.
  The directional derivative is maximized when $\vect{u}$ is in the
  same direction as $(\nabla f)(\vect{x})$ (assuming $(\nabla
  f)(\vect{x}) \ne \vect{0}$).
\end{example}

\begin{theorem}[Mean Value Theorem for Vector Functions]
  \label{thm:chap9:mean_value_thm_vector}
  Suppose $\map{f}{\text{convex open set } E \subset \R^n}{\R^m}$, f
  is differentiable in E, and there is a real number M such that
  $\norm{f'(\vect{x})} \le M$ for every $\vect{x} \in E$. Then
  \[
    |f(\vect{b}) - f(\vect{a})| \le M |\vect{b} - \vect{a}|
  \]
  for all $\vect{a} \in E, \vect{b} \in E$.
\end{theorem}
% Proof omitted.

\begin{corollary}
  \label{cor:chap9:zero_derivative_implies_constant}
  If, in addition, $f'(\vect{x}) = 0$ for all $\vect{x} \in E$ (where
  E is convex and open), then f is constant.
\end{corollary}
% Proof omitted.

\begin{definition}[Continuously Differentiable]
  \label{def:chap9:continuously_differentiable}
  A differentiable mapping $\map{f}{\text{open set } E \subset
  \R^n}{\R^m}$ is said to be \textbf{continuously differentiable} in
  E if $f'$ is a continuous mapping of E into $\mathcal{L}(\R^n,
  \R^m)$. That is, for every $\vect{x} \in E$ and $\epsilon > 0$,
  there exists $\delta > 0$ such that $\norm{f'(\vect{y}) -
  f'(\vect{x})} < \epsilon$ whenever $\vect{y} \in E$ and $|\vect{x}
  - \vect{y}| < \delta$.
  If this is so, we say f is a $\mathcal{C}'$-mapping, or that $f \in
  \mathcal{C}'(E)$.
\end{definition}

\begin{theorem}[Criterion for C']
  \label{thm:chap9:C1_iff_continuous_partials}
  Suppose $\map{f}{\text{open set } E \subset \R^n}{\R^m}$. Then $f
  \in \mathcal{C}'(E)$ if and only if the partial derivatives $D_j
  f_i$ exist and are continuous on E for $1 \le i \le m$, $1 \le j \le n$.
\end{theorem}
% Proof omitted.

% --- End of transcription chunk ---

% --- Previous content (up to Thm 9.21) from chapters/chapter9.tex above ---

\section{The Contraction Principle}

\begin{definition}[Contraction Mapping]
  \label{def:chap9:contraction_mapping}
  Let X be a metric space, with metric d. If $\map{\varphi}{X}{X}$
  and if there is a number $c < 1$ such that
  \[
    d(\varphi(x), \varphi(y)) \le c d(x, y)
  \]
  for all $x, y \in X$, then $\varphi$ is said to be a
  \textbf{contraction} of X into X.
\end{definition}

\begin{theorem}[Contraction Mapping Theorem / Banach Fixed-Point Theorem]
  \label{thm:chap9:contraction_mapping_thm}
  If X is a complete metric space, and if $\varphi$ is a contraction
  of X into X, then there exists one and only one $\vect{x} \in X$
  such that $\varphi(\vect{x}) = \vect{x}$. (In other words,
  $\varphi$ has a unique fixed point.)
\end{theorem}
% Proof omitted.

\section{The Inverse Function Theorem}

\begin{theorem}[Inverse Function Theorem]
  \label{thm:chap9:inverse_function_thm}
  Suppose f is a $\mathcal{C}'$-mapping of an open set $E \subset
  \R^n$ into $\R^n$, $f'(\vect{a})$ is invertible for some $\vect{a}
  \in E$, and $\vect{b} = f(\vect{a})$. Then
  \begin{enumerate}
    \item[(a)] there exist open sets U and V in $\R^n$ such that
      $\vect{a} \in U$, $\vect{b} \in V$, f is one-to-one on U, and $f(U) = V$;
    \item[(b)] if g is the inverse of f (which exists by (a)),
      defined in V by $g(f(\vect{x})) = \vect{x}$ for $\vect{x} \in
      U$, then $g \in \mathcal{C}'(V)$ and
      \begin{equation*} %\label{eq:chap9:inverse_derivative_formula}
        g'(\vect{y}) = \{ f'(g(\vect{y})) \}^{-1} \quad (\vect{y} \in V).
      \end{equation*}
  \end{enumerate}
\end{theorem}
% Proof omitted.

\begin{remark}[On the Proof of Theorem \ref{thm:chap9:inverse_function_thm}]
  \label{rem:chap9:ift_proof_remark}
  The full assumption $f \in \mathcal{C}'(E)$ is used only to
  establish the continuity of $g'$ (the final part of the proof). The
  existence and differentiability of g (down to the formula for
  $g'(\vect{y})$) relies only on the existence of $f'(\vect{x})$ for
  $\vect{x} \in E$, the invertibility of $f'(\vect{a})$, and the
  continuity of $f'$ *at the point* $\vect{a}$. (See A. Nijenhuis,
  Amer. Math. Monthly, vol. 81, 1974, pp. 969-980).
\end{remark}

% The following is an immediate consequence of part (a) of the inverse
% function theorem.

\begin{theorem}[Open Mapping Theorem]
  \label{thm:chap9:open_mapping_thm}
  If f is a $\mathcal{C}'$-mapping of an open set $E \subset \R^n$
  into $\R^n$ and if $f'(\vect{x})$ is invertible for every $\vect{x}
  \in E$, then $f(W)$ is an open subset of $\R^n$ for every open set
  $W \subset E$. (In other words, f is an open mapping.)
\end{theorem}

\begin{remark}[Local vs. Global Injectivity]
  \label{rem:chap9:local_vs_global_one_to_one}
  The hypotheses of Theorem \ref{thm:chap9:open_mapping_thm} ensure
  that f is locally one-to-one in E, but f need not be globally one-to-one in E.
\end{remark}

% --- End of revised transcription chunk ---

% --- Previous content (up to Rem 9.25) from chapters/chapter9.tex above ---

\section{The Implicit Function Theorem}

% Intro paragraph (condensed from text before 9.26)
The equation $f(x,y)=0$ relating real variables can often be solved
locally for $y$ in terms of $x$ if $\partial f/\partial y \ne 0$. The
Implicit Function Theorem generalizes this idea to systems of
equations and higher dimensions, showing that under appropriate
conditions on the derivative, a relation $f(\vect{x}, \vect{y}) =
\vect{0}$ implicitly defines $\vect{x}$ as a function of $\vect{y}$.
The linear version is considered first.

\subsection*{Notation for $\R^{n+m}$}
\label{sec:chap9:Rn+m_notation}
If $\vect{x} = (x_1, \dots, x_n) \in \R^n$ and $\vect{y} = (y_1,
\dots, y_m) \in \R^m$, we write $(\vect{x}, \vect{y})$ for the vector
$(x_1, \dots, x_n, y_1, \dots, y_m) \in \R^{n+m}$.

Every $A \in \mathcal{L}(\R^{n+m}, \R^n)$ can be split into two
linear transformations $A_x \in \mathcal{L}(\R^n)$ and $A_y \in
\mathcal{L}(\R^m, \R^n)$ defined by
\[
  A_x \vect{h} = A(\vect{h}, \vect{0}), \quad A_y \vect{k} =
  A(\vect{0}, \vect{k})
\]
for any $\vect{h} \in \R^n, \vect{k} \in \R^m$. Then $A(\vect{h},
\vect{k}) = A_x \vect{h} + A_y \vect{k}$.

\begin{theorem}[Linear Implicit Function Theorem]
  \label{thm:chap9:linear_ift}
  If $A \in \mathcal{L}(\R^{n+m}, \R^n)$ and if $A_x$ is invertible,
  then for every $\vect{k} \in \R^m$ there corresponds a unique
  $\vect{h} \in \R^n$ such that $A(\vect{h}, \vect{k}) = \vect{0}$.
  This $\vect{h}$ is given by $\vect{h} = -(A_x)^{-1} A_y \vect{k}$.
\end{theorem}
% Proof omitted.

\begin{remark}
  Theorem \ref{thm:chap9:linear_ift} states that the linear system of
  equations $A_x \vect{h} + A_y \vect{k} = \vect{0}$ can be solved
  uniquely for $\vect{h}$ in terms of $\vect{k}$ when $A_x$ is
  invertible, and the solution is a linear function of $\vect{k}$.
\end{remark}

\begin{theorem}[Implicit Function Theorem]
  \label{thm:chap9:implicit_function_thm}
  Let f be a $\mathcal{C}'$-mapping of an open set $E \subset
  \R^{n+m}$ into $\R^n$, such that $f(\vect{a}, \vect{b}) = \vect{0}$
  for some point $(\vect{a}, \vect{b}) \in E$. Put $A = f'(\vect{a},
  \vect{b})$ and assume that $A_x$ is invertible.

  Then there exist open sets $U \subset \R^{n+m}$ and $W \subset
  \R^m$, with $(\vect{a}, \vect{b}) \in U$ and $\vect{b} \in W$,
  having the following property:
  To every $\vect{y} \in W$ corresponds a unique $\vect{x}$ such that
  $(\vect{x}, \vect{y}) \in U$ and $f(\vect{x}, \vect{y}) = \vect{0}$.

  If this $\vect{x}$ is defined to be $g(\vect{y})$, then g is a
  $\mathcal{C}'$-mapping of W into $\R^n$, $g(\vect{b}) = \vect{a}$,
  \[
    f(g(\vect{y}), \vect{y}) = \vect{0} \quad (\vect{y} \in W),
  \]
  and
  \[
    g'(\vect{b}) = -(A_x)^{-1} A_y.
  \]
\end{theorem}
% Proof omitted.

\begin{remark}
  The function g is "implicitly" defined by $f(g(\vect{y}), \vect{y})
  = \vect{0}$. The condition that $A_x$ is invertible means that the
  Jacobian determinant $\frac{\partial(f_1, \dots,
  f_n)}{\partial(x_1, \dots, x_n)}$ evaluated at $(\vect{a},
  \vect{b})$ is non-zero. The theorem guarantees a local,
  continuously differentiable solution $\vect{x} = g(\vect{y})$ near
  $(\vect{a}, \vect{b})$. The derivative $g'(\vect{b})$ can be
  computed via matrix equation $A_x g'(\vect{b}) + A_y = 0$.
\end{remark}

\begin{example}[Application of IFT]
  \label{ex:chap9:ift_application}
  Consider $\map{f=(f_1, f_2)}{\R^5}{\R^2}$ given by
  \begin{align*}
    f_1(x_1, x_2, y_1, y_2, y_3) &= 2e^{x_1} + x_2 y_1 - 4y_2 + 3 \\
    f_2(x_1, x_2, y_1, y_2, y_3) &= x_2 \cos x_1 - 6x_1 + 2y_1 - y_3.
  \end{align*}
  Let $\vect{a}=(0, 1)$ and $\vect{b}=(3, 2, 7)$. Then $f(\vect{a},
  \vect{b}) = \vect{0}$.
  The derivative $A = f'(\vect{a}, \vect{b})$ has components $A_x$
  and $A_y$ such that $A_x$ is invertible (its matrix is $[A_x] =
    \begin{pmatrix} 2 & 3 \\ -6 & 1
  \end{pmatrix}$).
  The Implicit Function Theorem guarantees the existence of a
  $\mathcal{C}'$-mapping $\vect{x} = g(\vect{y})$ defined in a
  neighborhood W of $\vect{b}=(3, 2, 7)$, such that $g(\vect{b}) =
  \vect{a}=(0, 1)$ and $f(g(\vect{y}), \vect{y}) = \vect{0}$ for
  $\vect{y} \in W$.
  The derivative $g'(\vect{b})$ is given by the $2 \times 3$ matrix:
  \[
    [g'(3, 2, 7)] =
    \begin{pmatrix}
      1/4 & 1/5 & -3/20 \\
      -1/2 & 6/5 & 1/10
    \end{pmatrix}.
  \]
  (This matrix gives the partial derivatives $\partial g_i / \partial
  y_k$ at $\vect{b}$.)
\end{example}

% --- End of transcription chunk ---

% --- Previous content (up to Ex 9.29) from chapters/chapter9.tex above ---

\section{The Rank Theorem}

% Introductory text condensed/omitted.

\begin{definition}[Null Space, Range, Rank]
  \label{def:chap9:null_space_range_rank}
  Suppose $A \in \mathcal{L}(X, Y)$.
  \begin{itemize}
    \item The \textbf{null space} of A, $\mathcal{N}(A)$, is the set
      of all $\vect{x} \in X$ such that $A\vect{x} = \vect{0}$.
      ($\mathcal{N}(A)$ is a vector space in X).
    \item The \textbf{range} of A, $\mathcal{R}(A)$, is the set of
      all $A\vect{x}$ for $\vect{x} \in X$. ($\mathcal{R}(A)$ is a
      vector space in Y).
    \item The \textbf{rank} of A is $\rank A = \dim \mathcal{R}(A)$.
  \end{itemize}
  Note: $A \in \mathcal{L}(\R^n)$ is invertible iff $\rank A = n$. If
  $\rank A = 0$, then $A=0$ and $\mathcal{N}(A)=X$.
\end{definition}

\begin{definition}[Projection]
  \label{def:chap9:projection}
  An operator $P \in \mathcal{L}(X)$ is said to be a
  \textbf{projection} in X if $P^2 = P$ (i.e., $P(P\vect{x}) =
  P\vect{x}$ for all $\vect{x} \in X$).
  Properties:
  \begin{itemize}
    \item[(a)] If P is a projection in X, then every $\vect{x} \in X$
      has a unique representation $\vect{x} = \vect{x}_1 +
      \vect{x}_2$ where $\vect{x}_1 \in \mathcal{R}(P)$ and
      $\vect{x}_2 \in \mathcal{N}(P)$. (Specifically, $\vect{x}_1 =
      P\vect{x}$ and $\vect{x}_2 = \vect{x} - P\vect{x}$).
    \item[(b)] If X is finite-dimensional and $X_1$ is a subspace of
      X, there exists a projection P in X with $\mathcal{R}(P) =
      X_1$. (This can be constructed using a basis of X that contains
      a basis of $X_1$).
  \end{itemize}
\end{definition}

\begin{theorem}[Rank Theorem]
  \label{thm:chap9:rank_theorem}
  Suppose m, n, r are nonnegative integers, $m \ge r, n \ge r$. Let F
  be a $\mathcal{C}'$-mapping of an open set $E \subset \R^n$ into
  $\R^m$, and assume $\rank F'(\vect{x}) = r$ for every $\vect{x} \in E$.
  Fix $\vect{a} \in E$, put $A = F'(\vect{a})$, let $Y_1 =
  \mathcal{R}(A)$, and let P be a projection in $\R^m$ whose range is
  $Y_1$. Let $Y_2 = \mathcal{N}(P)$.

  Then there are open sets U and V in $\R^n$ with $\vect{a} \in U, U
  \subset E$, and there is a 1-1 $\mathcal{C}'$-mapping H of V onto U
  (whose inverse is also of class $\mathcal{C}'$) such that
  \[
    F(H(\vect{x})) = A\vect{x} + \varphi(A\vect{x}) \quad (\vect{x} \in V)
  \]
  where $\varphi$ is a $\mathcal{C}'$-mapping of the open set $A(V)
  \subset Y_1$ into $Y_2$.
\end{theorem}
% Proof omitted.

\begin{remark}[Geometric Interpretation of Rank Theorem]
  \label{rem:chap9:rank_thm_geometry}
  Theorem \ref{thm:chap9:rank_theorem} implies that locally, the
  image $F(U)$ is essentially an r-dimensional surface parameterized
  by $A(V) \subset Y_1$. Specifically, any point $\vect{y} \in F(U)$
  can be written as $\vect{y} = P\vect{y} + \varphi(P\vect{y})$,
  where $P\vect{y} \in A(V)$. The projection P restricted to $F(U)$
  is a 1-1 map onto $A(V)$.
  The level sets of $F \circ H$ in V are the same as the level sets
  of the linear map A, which are translates of $\mathcal{N}(A)$ (an
  $(n-r)$-dimensional space). Consequently, the level sets of F in U
  are $(n-r)$-dimensional surfaces obtained by applying H to these flat sets.
\end{remark}

% --- End of transcription chunk ---

% --- Previous content (up to Rem 9.32) from chapters/chapter9.tex above ---

\section{Determinants}

% Introductory text condensed/omitted.

\begin{definition}[Determinant]
  \label{def:chap9:determinant}
  If $(j_1, \dots, j_n)$ is an ordered n-tuple of integers, define
  \[
    s(j_1, \dots, j_n) = \prod_{p<q} \operatorname{sgn}(j_q - j_p),
  \]
  where $\operatorname{sgn} x = 1$ if $x > 0$, $\operatorname{sgn} x
  = -1$ if $x < 0$, and $\operatorname{sgn} 0 = 0$. (Thus $s(\dots)$
  is 1, -1, or 0, and changes sign if any two $j$'s are interchanged).

  Let $[A]$ be the $n \times n$ matrix of a linear operator A on
  $\R^n$ relative to the standard basis $\{ \vect{e}_1, \dots,
  \vect{e}_n \}$, with entry $a(i, j)$ in the i-th row and j-th
  column. The \textbf{determinant} of $[A]$ is
  \[
    \det[A] = \sum s(j_1, \dots, j_n) a(1, j_1) a(2, j_2) \cdots a(n, j_n).
  \]
  The sum extends over all $n^n$ ordered n-tuples $(j_1, \dots, j_n)$
  with $1 \le j_r \le n$.

  If $\vect{x}_j = \sum_i a(i, j) \vect{e}_i$ are the column vectors
  of $[A]$, we also write $\det(\vect{x}_1, \dots, \vect{x}_n) = \det[A]$.
\end{definition}

\begin{theorem}[Properties of Determinants]
  \label{thm:chap9:determinant_properties}
  ~ % Using enumerate for parts (a) through (d)
  \begin{enumerate}
    \item[(a)] If $\id$ is the identity operator, $\det[\id] =
      \det(\vect{e}_1, \dots, \vect{e}_n) = 1$.
    \item[(b)] $\det(\vect{x}_1, \dots, \vect{x}_n)$ is a linear
      function of each column vector $\vect{x}_j$ if the others are held fixed.
    \item[(c)] If $[A]_1$ is obtained from $[A]$ by interchanging two
      columns, then $\det[A]_1 = -\det[A]$.
    \item[(d)] If $[A]$ has two equal columns, then $\det[A] = 0$.
  \end{enumerate}
\end{theorem}
% Proof omitted.

\begin{theorem}[Determinant of a Product]
  \label{thm:chap9:determinant_product}
  If $[A]$ and $[B]$ are $n \times n$ matrices, then $\det([B][A]) =
  \det[B] \det[A]$.
\end{theorem}
% Proof omitted.

\begin{theorem}[Invertibility and Determinant]
  \label{thm:chap9:invertibility_iff_det_nonzero}
  A linear operator A on $\R^n$ is invertible if and only if $\det[A] \ne 0$.
\end{theorem}
% Proof omitted.

\begin{remark}[Determinant Independent of Basis]
  \label{rem:chap9:det_independent_of_basis}
  The determinant of the matrix representation of a linear operator A
  on $\R^n$ does not depend on the basis used to construct the
  matrix. Thus, $\det A$ is well-defined for $A \in
  \mathcal{L}(\R^n)$. (This follows from $\det([B][A]_U) =
    \det([A][B])$ where $[A]_U$ is the matrix w.r.t basis U and B is
  the change-of-basis matrix, see Thm \ref{thm:chap9:determinant_product}).
\end{remark}

\subsection*{Jacobians}
\label{sec:chap9:jacobians}

If $\map{f}{\text{open set } E \subset \R^n}{\R^n}$ is differentiable
at $\vect{x} \in E$, the determinant of the linear operator
$f'(\vect{x})$ is called the \textbf{Jacobian} of f at $\vect{x}$. In symbols,
\[
  J_f(\vect{x}) = \det f'(\vect{x}).
\]
If $\vect{y} = f(\vect{x})$, i.e., $(y_1, \dots, y_n) = f(x_1, \dots,
x_n)$, the notation
\[
  \frac{\partial(y_1, \dots, y_n)}{\partial(x_1, \dots, x_n)}
\]
is also used for $J_f(\vect{x})$.
The condition $J_f(\vect{a}) \ne 0$ is the crucial hypothesis for the
invertibility of $f'(\vect{a})$ in the Inverse Function Theorem (Thm
\ref{thm:chap9:inverse_function_thm}). Similarly, the invertibility
condition on $A_x$ in the Implicit Function Theorem (Thm
\ref{thm:chap9:implicit_function_thm}) amounts to requiring that the
corresponding Jacobian $\frac{\partial(f_1, \dots,
f_n)}{\partial(x_1, \dots, x_n)}$ is non-zero at $(\vect{a}, \vect{b})$.

% --- End of transcription chunk ---

% --- Previous content (up to Sec 9.38) from chapters/chapter9.tex above ---

\section{Derivatives of Higher Order}

\begin{definition}[Higher-Order Partial Derivatives, Class $\mathcal{C}''$]
  \label{def:chap9:higher_partials_C2}
  Suppose f is a real function defined in an open set $E \subset
  \R^n$, with partial derivatives $D_1 f, \dots, D_n f$. If the
  functions $D_j f$ are themselves differentiable, then the
  \textbf{second-order partial derivatives} of f are defined by
  \[
    D_{ij}f = D_i D_j f \quad (i, j = 1, \dots, n).
  \]
  If all these functions $D_{ij}f$ are continuous in E, we say that f
  is of \textbf{class $\mathcal{C}''$} in E, or that $f \in \mathcal{C}''(E)$.

  A mapping $\map{f}{E}{\R^m}$ is of class $\mathcal{C}''$ if each
  component of f is of class $\mathcal{C}''$.
  (Note: It can happen that $D_{ij}f \ne D_{ji}f$ if continuity is not assumed).
\end{definition}

% For simplicity, the next two theorems are stated for $n=2$.

\begin{theorem}[Mean Value Theorem for Mixed Partials]
  \label{thm:chap9:mvt_mixed_partials}
  Suppose f is defined in an open set $E \subset \R^2$, and $D_1 f$
  and $D_{21}f$ exist at every point of E. Suppose $Q \subset E$ is a
  closed rectangle with sides parallel to the coordinate axes, having
  $(\textit{a}, \textit{b})$ and $(a+h, b+k)$ as opposite vertices
  $(h \ne 0, k \ne 0)$. Put
  \[
    \Delta(f, Q) = f(a+h, b+k) - f(a+h, b) - f(a, b+k) + f(a, b).
  \]
  Then there is a point $(x, y)$ in the interior of Q such that
  $\Delta(f, Q) = hk (D_{21}f)(x, y)$.
\end{theorem}
% Proof omitted.

\begin{theorem}[Equality of Mixed Partials]
  \label{thm:chap9:equality_mixed_partials}
  Suppose f is defined in an open set $E \subset \R^2$, suppose that
  $D_1 f$, $D_{21}f$, and $D_2 f$ exist at every point of E, and
  $D_{21}f$ is continuous at some point $(a, b) \in E$. Then
  $D_{12}f$ exists at $(a, b)$ and $(D_{12}f)(a, b) = (D_{21}f)(a, b)$.
\end{theorem}
% Proof omitted.

\begin{corollary}
  \label{cor:chap9:C2_implies_mixed_partials_equal}
  If $f \in \mathcal{C}''(E)$ for some open set $E \subset \R^n$,
  then $D_{ij}f = D_{ji}f$ in E for all $i, j = 1, \dots, n$.
\end{corollary}

\section{Differentiation of Integrals}

% Question: Under what conditions is $\frac{d}{dt} \int_a^b
% \varphi(x, t) dx = \int_a^b \frac{\partial \varphi}{\partial t}(x, t) dx$?

\begin{theorem}[Differentiation Under the Integral Sign]
  \label{thm:chap9:diff_under_integral}
  Suppose
  \begin{itemize}
    \item[(a)] $\varphi(x, t)$ is defined for $a \le x \le b$, $c \le t \le d$;
    \item[(b)] $\alpha$ is an increasing function on $[a, b]$;
    \item[(c)] $\varphi^t \in \mathcal{R}(\alpha)$ for every $t \in
      [c, d]$ (where $\varphi^t(x) = \varphi(x, t)$);
    \item[(d)] $c < s < d$, and $D_2 \varphi = \partial \varphi /
      \partial t$ satisfies: for every $\epsilon > 0$ there exists
      $\delta > 0$ such that $|(D_2 \varphi)(x, t) - (D_2 \varphi)(x,
      s)| < \epsilon$ for all $x \in [a, b]$ and $t \in (s-\delta,
      s+\delta)$. (This holds if $D_2 \varphi$ is uniformly
        continuous in x for t near s, e.g., if $D_2 \varphi$ is
      continuous on $[a,b] \times [c,d]$).
  \end{itemize}
  Define $f(t) = \int_a^b \varphi(x, t) d\alpha(x)$ for $c \le t \le
  d$. Then $(D_2 \varphi)^s \in \mathcal{R}(\alpha)$, $f'(s)$ exists, and
  \[
    f'(s) = \int_a^b (D_2 \varphi)(x, s) d\alpha(x).
  \]
\end{theorem}
% Proof omitted.

\begin{example}[Evaluating $\int_{-\infty}^{\infty} e^{-x^2} \cos(xt) dx$]
  \label{ex:chap9:gaussian_fourier_transform}
  Let $f(t) = \int_{-\infty}^{\infty} e^{-x^2} \cos(xt) dx$ for $t
  \in \R$. The integral converges absolutely. Differentiating under
  the integral sign (justified by uniform convergence arguments
  similar to Thm \ref{thm:chap9:diff_under_integral}) yields $f'(t) =
  -\int_{-\infty}^{\infty} x e^{-x^2} \sin(xt) dx$. Integration by
  parts shows $f'(t) = -t f(t) / 2$. Solving this differential
  equation with the initial condition $f(0) = \int_{-\infty}^{\infty}
  e^{-x^2} dx = \sqrt{\pi}$ (from Sec 8.21) gives the result:
  \[
    \int_{-\infty}^{\infty} e^{-x^2} \cos(xt) dx = \sqrt{\pi}
    \exp\left(-\frac{t^2}{4}\right).
  \]
\end{example}

% --- End of transcription chunk ---
