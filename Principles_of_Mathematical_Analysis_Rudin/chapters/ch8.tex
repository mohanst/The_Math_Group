% --- chapters/chapter8.tex ---
% Generated by Gemini (Google AI) on 2025-04-05.
% Based on principles_of_mathematical_analysis_walter_rudin_ch_8.pdf
% Assumes macros from user-provided macros.tex (incl. Ch 8 additions)
% are defined.
% Proofs are omitted as requested. Using \norm{} for vector norms if applicable.

\chapter{Some Special Functions}
\label{chap:rudin8}

\section{Power Series}
\label{sec:chap8:power_series}

% In this section we shall derive some properties of functions which
% are represented
% by power series, i.e., functions of the form
% \begin{equation} \label{eq:chap8:ps_form1}
% f(x) = \sum_{n=0}^\infty c_n x^n
% \end{equation}
% or, more generally,
% \begin{equation} \label{eq:chap8:ps_form2}
% f(x) = \sum_{n=0}^\infty c_n (x-a)^n.
% \end{equation}
% These are called analytic functions.
% We shall restrict ourselves to real values of x. Instead of circles of con-
% vergence (see Theorem 3.39) we shall therefore encounter intervals of conver-
% gence.
% If (1) converges for all x in (-R, R), for some R>0 (R may be +infinity),
% we say that f is expanded in a power series about the point x=0. Similarly, if
% (2) converges for |x-a|<R, f is said to be expanded in a power series about
% the point x=a. As a matter of convenience, we shall often take a=0 without
% any loss of generality.

\begin{theorem}[Properties of Power Series] % Theorem 8.1 from Ch 8 PDF
  \label{thm:chap8:power_series_props}
  Suppose the series
  \begin{equation} \label{eq:chap8:ps_basic}
    \sum_{n=0}^\infty c_n x^n
  \end{equation}
  converges for $\abs{x} < R$, and define
  \begin{equation} \label{eq:chap8:ps_func_def}
    f(x) = \sum_{n=0}^\infty c_n x^n \quad (\abs{x} < R).
  \end{equation}
  Then \eqref{eq:chap8:ps_basic} converges uniformly on
  $[-R+\epsilon, R-\epsilon]$ no matter which $\epsilon > 0$ is
  chosen. The function f is continuous and differentiable in $(-R, R)$, and
  \begin{equation} \label{eq:chap8:ps_derivative}
    f'(x) = \sum_{n=1}^\infty n c_n x^{n-1} \quad (\abs{x} < R).
  \end{equation}
  % Proof Omitted (Uses Thm 7.10, Thm 7.17, Thm 5.2)
\end{theorem}

\begin{corollary}[Higher Derivatives, Coefficients] % Corollary to
  % 8.1 from Ch 8 PDF
  \label{cor:chap8:ps_coeffs}
  Under the hypotheses of \autoref{thm:chap8:power_series_props}, f
  has derivatives of all orders in $(-R, R)$, which are given by
  \begin{equation} \label{eq:chap8:ps_higher_deriv}
    f^{(k)}(x) = \sum_{n=k}^\infty n(n-1)\dots(n-k+1) c_n x^{n-k}.
  \end{equation}
  In particular,
  \begin{equation} \label{eq:chap8:ps_coeffs_formula}
    f^{(k)}(0) = k! c_k \quad (k=0, 1, 2, \dots).
  \end{equation}
  (Here $f^{(0)}$ means f, and $f^{(k)}$ is the k-th derivative of f,
  for $k=1, 2, 3, \dots$.)
  % Proof Omitted (Applies Thm 8.1 successively)
\end{corollary}

% Note: Rudin discusses uniqueness and the possibility of non-convergence
% of the Taylor series generated by f^(k)(0) back to f.

% If the series (3) converges at an endpoint, say at x=R, then f is continuous
% not only in (-R, R), but also at x=R. This follows from Abel's theorem (for
% simplicity of notation, we take R=1):

\begin{theorem}[Abel's Theorem] % Theorem 8.2 from Ch 8 PDF
  \label{thm:chap8:abels_theorem}
  Suppose $\sum c_n$ converges. Put
  \[ f(x) = \sum_{n=0}^\infty c_n x^n \quad (-1 < x < 1). \]
  Then
  \begin{equation} \label{eq:chap8:abel_limit}
    \lim_{x \to 1^-} f(x) = \sum_{n=0}^\infty c_n.
  \end{equation}
  (Limit is taken as x approaches 1 from the left).
  % Proof Omitted (Uses summation by parts for series)
\end{theorem}

% --- End of content chunk ---

% --- Content from Thm 8.3 to Thm 8.5 (Proofs Omitted) to append ---

% Note: Rudin includes an application of Abel's Theorem to prove
% Mertens' Theorem
% (Cauchy product) again here before Theorem 8.3.

\begin{theorem}[Interchange of Summation] % Theorem 8.3 from Ch 8 PDF
  \label{thm:chap8:summation_interchange}
  Given a double sequence $\{a_{ij}\}$, $i=1, 2, 3, \dots, j=1, 2, 3,
  \dots$, suppose that
  \begin{equation} \label{eq:chap8:abs_row_sum}
    \sum_{j=1}^\infty \abs{a_{ij}} = b_i \quad (i=1, 2, 3, \dots)
  \end{equation}
  and $\sum b_i$ converges. Then
  \begin{equation} \label{eq:chap8:sum_interchange_result}
    \sum_{i=1}^\infty \sum_{j=1}^\infty a_{ij} = \sum_{j=1}^\infty
    \sum_{i=1}^\infty a_{ij}.
  \end{equation}
  % Proof Omitted (Uses Thm 7.11 on appropriately defined functions
  % on a countable set)
\end{theorem}

\begin{theorem}[Taylor's Theorem for Power Series] % Theorem 8.4 from Ch 8 PDF
  \label{thm:chap8:ps_taylor}
  Suppose
  \[ f(x) = \sum_{n=0}^\infty c_n x^n, \]
  the series converging in $\abs{x} < R$. If $-R < a < R$, then f can
  be expanded in a power series about the point $x=a$ which converges
  in $\abs{x-a} < R - \abs{a}$, and
  \begin{equation} \label{eq:chap8:taylor_expansion}
    f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x-a)^n \quad
    (\abs{x-a} < R - \abs{a}).
  \end{equation}
  % Proof Omitted (Uses binomial expansion and Thm 8.3)
\end{theorem}

\begin{theorem}[Uniqueness Theorem for Power Series] % Theorem 8.5 from Ch 8 PDF
  \label{thm:chap8:ps_uniqueness}
  Suppose the series $\sum a_n x^n$ and $\sum b_n x^n$ converge in
  the segment $S = (-R, R)$. Let E be the set of all $x \in S$ at which
  \begin{equation} \label{eq:chap8:ps_equality}
    \sum_{n=0}^\infty a_n x^n = \sum_{n=0}^\infty b_n x^n.
  \end{equation}
  If E has a limit point in S, then $a_n = b_n$ for $n=0, 1, 2,
  \dots$. Hence \eqref{eq:chap8:ps_equality} holds for all $x \in S$.
  % Proof Omitted (Uses properties of analytic functions, Thm 8.4, continuity)
\end{theorem}

% --- End of content chunk ---

% --- Content for Exponential/Log Functions (incl Thm 8.6, Proofs
% Omitted) to append ---

\section{The Exponential and Logarithmic Functions}
\label{sec:chap8:exp_log}

We define
\begin{equation} \label{eq:chap8:exp_def_series}
  E(z) = \sum_{n=0}^\infty \frac{z^n}{n!}.
\end{equation}
The ratio test shows that this series converges for every complex z.
Applying multiplication of absolutely convergent series
(\autoref{thm:chap3:mertens_cauchy_product} applied appropriately or
  direct multiplication argument from text referencing Thm 3.50 in 2nd
Ed / 3.47 in 3rd Ed), we obtain the important addition formula
\begin{equation} \label{eq:chap8:exp_add_formula}
  E(z+w) = E(z)E(w) \quad (z, w \text{ complex}).
\end{equation}
One consequence is $E(z)E(-z) = E(0) = 1$, which shows that $E(z) \ne
0$ for all z.
Let $e = E(1)$ (this is the number from
\autoref{def:chap3:e_definition}). Iteration of
\eqref{eq:chap8:exp_add_formula} gives $E(n) = e^n$ for positive
integers n. This extends to rational p, $E(p) = e^p$.
We now define, for any real x,
\begin{equation} \label{eq:chap8:euler_num_power}
  e^x = E(x).
\end{equation}
This is the exponential function. It satisfies $E'(x) = E(x)$ (from
\eqref{eq:chap8:exp_def_series} and \autoref{thm:chap8:power_series_props}).

\begin{theorem}[Properties of the Exponential Function] % Theorem 8.6
  % from Ch 8 PDF
  \label{thm:chap8:exp_properties}
  Let $e^x$ be defined on $\R^1$ by \eqref{eq:chap8:euler_num_power}
  and \eqref{eq:chap8:exp_def_series}. Then
  (a) $e^x$ is continuous and differentiable for all x;
  (b) $(e^x)' = e^x$;
  (c) $e^x$ is a strictly increasing function of x, and $e^x > 0$;
  (d) $e^{x+y} = e^x e^y$;
  (e) $e^x \to +\infty$ as $x \to +\infty$, $e^x \to 0$ as $x \to -\infty$;
  (f) $\lim_{x \to +\infty} x^n e^{-x} = 0$, for every n.
  % Proof Omitted
\end{theorem}

Since E is strictly increasing and differentiable on $\R^1$, it has
an inverse function L which is also strictly increasing and
differentiable and whose domain is $E(\R^1) = (0, \infty)$. L is defined by
\begin{equation} \label{eq:chap8:log_def_1}
  E(L(y)) = y \quad (y > 0),
\end{equation}
or, equivalently, by
\begin{equation} \label{eq:chap8:log_def_2}
  L(E(x)) = x \quad (x \text{ real}).
\end{equation}
Differentiating \eqref{eq:chap8:log_def_2} using the chain rule
(\autoref{thm:chap5:chain_rule}) gives $L'(E(x)) \cdot E'(x) = 1$.
Since $E'(x) = E(x)$, we have $L'(E(x)) \cdot E(x) = 1$. Writing $y =
E(x)$, this gives us
\begin{equation} \label{eq:chap8:log_deriv}
  L'(y) = \frac{1}{y} \quad (y > 0).
\end{equation}
Since $E(0)=1$, we have $L(1)=0$. By \eqref{eq:chap8:log_deriv} and
the Fundamental Theorem of Calculus (\autoref{thm:chap6:ftc2}),
\begin{equation} \label{eq:chap8:log_integral_def}
  L(y) = \int_1^y \frac{dx}{x}.
\end{equation}
The customary notation for $L(x)$ is $\log x$.
From the addition formula \eqref{eq:chap8:exp_add_formula} and the
definition, $L(uv) = L(E(x)E(y)) = L(E(x+y)) = x+y = L(u)+L(v)$, so
\begin{equation} \label{eq:chap8:log_product_rule}
  \log(uv) = \log u + \log v \quad (u>0, v>0).
\end{equation}
We now define $x^\alpha$ for any real $\alpha$ and any $x>0$, by
\begin{equation} \label{eq:chap8:power_def}
  x^\alpha = E(\alpha L(x)) = e^{\alpha \log x}.
\end{equation}
This is consistent with earlier definitions for rational $\alpha$.
Differentiating \eqref{eq:chap8:power_def} using the chain rule gives
\begin{equation} \label{eq:chap8:power_deriv}
  (x^\alpha)' = E(\alpha L(x)) \cdot (\alpha L'(x)) = e^{\alpha \log
  x} \cdot \frac{\alpha}{x} = x^\alpha \cdot \frac{\alpha}{x} =
  \alpha x^{\alpha-1}.
\end{equation}
Also, for every $\alpha > 0$,
\begin{equation} \label{eq:chap8:log_power_limit}
  \lim_{x \to +\infty} x^{-\alpha} \log x = 0.
\end{equation}
% Proof Omitted

% --- End of content chunk ---

% --- Content for Trigonometric Functions (incl Thm 8.7, Proofs
% Omitted) to append ---

\section{The Trigonometric Functions}
\label{sec:chap8:trig_functions}

Let us define
\begin{align}
  C(x) &= \frac{1}{2}[E(ix) + E(-ix)], \label{eq:chap8:def_C} \\
  S(x) &= \frac{1}{2i}[E(ix) - E(-ix)]. \label{eq:chap8:def_S}
\end{align}
We shall show that $C(x)$ and $S(x)$ coincide with the functions
$\cos x$ and $\sin x$, whose definition is usually based on geometric
considerations.
By \eqref{eq:chap8:exp_def_series}, $E(\bar{z}) = \overline{E(z)}$.
Hence \eqref{eq:chap8:def_C} and \eqref{eq:chap8:def_S} show that
$C(x)$ and $S(x)$ are real for real x. Also,
\begin{equation} \label{eq:chap8:euler_formula}
  E(ix) = C(x) + iS(x).
\end{equation}
Thus $C(x)$ and $S(x)$ are the real and imaginary parts,
respectively, of $E(ix)$, if x is real. By $E(z)E(-z)=1$, we have
\[ \abs{E(ix)}^2 = E(ix) \overline{E(ix)} = E(ix)E(-ix) = 1, \]
so that
\begin{equation} \label{eq:chap8:exp_ix_modulus}
  \abs{E(ix)} = 1 \quad (x \text{ real}).
\end{equation}
From \eqref{eq:chap8:def_C} and \eqref{eq:chap8:def_S} we can read
off that $C(0)=1, S(0)=0$, and $E'(z)=E(z)$ shows that
\begin{equation} \label{eq:chap8:trig_derivs}
  C'(x) = -S(x), \quad S'(x) = C(x).
\end{equation}
We assert that there exist positive numbers x such that $C(x)=0$. For
suppose this is not so. Since $C(0)=1$, it then follows that $C(x)>0$
for all $x>0$, hence $S'(x)>0$, by \eqref{eq:chap8:trig_derivs},
hence S is strictly increasing; and since $S(0)=0$, we have $S(x)>0$
if $x>0$. Hence if $0<x<y$, $\int_x^y S(t) dt = C(x) - C(y)$. Since
$S(t) \ge S(x)$ for $t \in [x, y]$, this gives $S(x)(y-x) \le
C(x)-C(y) \le 2$ (by \eqref{eq:chap8:euler_formula} and
\eqref{eq:chap8:exp_ix_modulus}). This cannot be true for large $y$.
Let $x_0$ be the smallest positive number such that $C(x_0)=0$. This
exists, since the set of zeros of a continuous function is closed,
and $C(0) \ne 0$. We define the number $\pi$ by
\begin{equation} \label{eq:chap8:pi_definition}
  \pi = 2x_0.
\end{equation}
Then $C(\pi/2)=0$, and \eqref{eq:chap8:exp_ix_modulus} shows that
$S(\pi/2)=\pm 1$. Since $S$ is increasing in $(0, \pi/2)$, $S(\pi/2)=1$. Thus
\[ E(\pi i / 2) = i, \]
and the addition formula gives
\begin{align}
  E(\pi i) &= i^2 = -1, \label{eq:chap8:exp_ipi} \\
  E(2\pi i) &= (-1)^2 = 1; \label{eq:chap8:exp_2ipi}
\end{align}
hence
\begin{equation} \label{eq:chap8:exp_periodicity}
  E(z + 2\pi i) = E(z)E(2\pi i) = E(z) \quad (z \text{ complex}).
\end{equation}

\begin{theorem}[Properties of E, C, S] % Theorem 8.7 from Ch 8 PDF
  \label{thm:chap8:trig_props}
  (a) The function E is periodic, with period $2\pi i$.
  (b) The functions C and S are periodic, with period $2\pi$.
  (c) If $0 < t < 2\pi$, then $E(it) \ne 1$.
  (d) If z is a complex number with $\abs{z}=1$, there is a unique t
  in $[0, 2\pi)$ such that $E(it)=z$.
  % Proof Omitted
\end{theorem}

% Note: Rudin then discusses the geometric interpretation, connection
% to arc length via Thm 6.27,
% and alternative non-geometric approaches.

% --- End of content chunk ---

% --- Content from Thm 8.8 to Def 8.10 (Proofs Omitted) to append ---

\section{The Algebraic Completeness of the Complex Field}
\label{sec:chap8:algebraic_completeness}

% We are now in a position to give a simple proof of the fact that the complex
% field is algebraically complete, that is to say, that every
% nonconstant polynomial
% with complex coefficients has a complex root.

\begin{theorem}[Fundamental Theorem of Algebra] % Theorem 8.8 from Ch 8 PDF
  \label{thm:chap8:fta}
  Suppose $a_0, \dots, a_n$ are complex numbers, $n \ge 1$, $a_n \ne 0$,
  \[ P(z) = \sum_{k=0}^n a_k z^k. \]
  Then $P(z)=0$ for some complex number z.
  % Proof Omitted (Uses minimum modulus principle argument, Thm 4.16
  % - actually refers to 4.15 in text body, Thm 8.7(d))
\end{theorem}

\section{Fourier Series}
\label{sec:chap8:fourier_series}

\begin{definition}[Trigonometric Polynomial/Series, Fourier
  Series/Coeffs] % Definition 8.9 from Ch 8 PDF
  \label{def:chap8:trig_poly_series}
  A trigonometric polynomial is a finite sum of the form
  \begin{equation} \label{eq:chap8:trig_poly_cos_sin}
    f(x) = a_0 + \sum_{n=1}^N (a_n \cos(nx) + b_n \sin(nx)) \quad (x
    \text{ real}),
  \end{equation}
  where $a_0, \dots, a_N, b_1, \dots, b_N$ are complex numbers. On
  account of the identities \eqref{eq:chap8:def_C} and
  \eqref{eq:chap8:def_S} [or Euler's formula
  \eqref{eq:chap8:euler_formula}], \eqref{eq:chap8:trig_poly_cos_sin}
  can also be written in the form
  \begin{equation} \label{eq:chap8:trig_poly_exp}
    f(x) = \sum_{n=-N}^N c_n e^{inx} \quad (x \text{ real}),
  \end{equation}
  which is more convenient for most purposes. It is clear that every
  trigonometric polynomial is periodic, with period $2\pi$.
  If n is a nonzero integer, $e^{inx}$ is the derivative of
  $e^{inx}/(in)$, which also has period $2\pi$. Hence
  \begin{equation} \label{eq:chap8:exp_orthogonality}
    \avgint{e^{inx}} = \frac{1}{2\pi} \int_{-\pi}^{\pi} e^{inx} \, dx =
    \begin{cases} 1 & (\text{if } n=0), \\ 0 & (\text{if } n = \pm 1,
      \pm 2, \dots).
    \end{cases}
  \end{equation}
  Let us multiply \eqref{eq:chap8:trig_poly_exp} by $e^{-imx}$, where
  m is an integer; if we integrate the product,
  \eqref{eq:chap8:exp_orthogonality} shows that
  \begin{equation} \label{eq:chap8:trig_poly_coeffs}
    c_m = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) e^{-imx} \, dx =
    \avgint{f(x) e^{-imx}}
  \end{equation}
  for $\abs{m} \le N$. If $\abs{m} > N$, the integral in
  \eqref{eq:chap8:trig_poly_coeffs} is 0.
  The trigonometric polynomial f, given by
  \eqref{eq:chap8:trig_poly_exp}, is real if and only if $c_{-n} =
  \overline{c_n}$ for $n=0, \dots, N$.

  In agreement with \eqref{eq:chap8:trig_poly_exp}, we define a
  trigonometric series to be a series of the form
  \begin{equation} \label{eq:chap8:trig_series}
    \sum_{n=-\infty}^\infty c_n e^{inx} \quad (x \text{ real});
  \end{equation}
  the Nth partial sum of \eqref{eq:chap8:trig_series} is defined to
  be the right side of \eqref{eq:chap8:trig_poly_exp}.
  If f is an integrable function on $[-\pi, \pi]$, the numbers $c_m$
  defined by \eqref{eq:chap8:trig_poly_coeffs} for all integers m are
  called the Fourier coefficients of f, and the series
  \eqref{eq:chap8:trig_series} formed with these coefficients is
  called the Fourier series of f.
\end{definition}

% Note: Rudin discusses the history and importance of Fourier series here.

\begin{definition}[Orthogonal Systems] % Definition 8.10 from Ch 8 PDF
  \label{def:chap8:orthogonal_systems}
  Let $\{\phi_n\}$ ($n=1, 2, 3, \dots$) be a sequence of complex
  functions on $[a, b]$, such that
  \begin{equation} \label{eq:chap8:orthogonality_condition}
    \int_a^b \phi_n(x) \overline{\phi_m(x)} \, dx = 0 \quad (n \ne m).
  \end{equation}
  Then $\{\phi_n\}$ is said to be an orthogonal system of functions
  on $[a, b]$. If, in addition,
  \begin{equation} \label{eq:chap8:orthonormality_condition}
    \int_a^b \abs{\phi_n(x)}^2 \, dx = 1
  \end{equation}
  for all n, $\{\phi_n\}$ is said to be orthonormal.
  For example, the functions $(2\pi)^{-1/2} e^{inx}$ form an
  orthonormal system on $[-\pi, \pi]$. So do the real functions
  \[ \frac{1}{\sqrt{2\pi}}, \frac{\cos x}{\sqrt{\pi}}, \frac{\sin
    x}{\sqrt{\pi}}, \frac{\cos(2x)}{\sqrt{\pi}},
  \frac{\sin(2x)}{\sqrt{\pi}}, \dots \]
  If $\{\phi_n\}$ is orthonormal on $[a, b]$ and if
  \begin{equation} \label{eq:chap8:gen_fourier_coeff}
    c_n = \int_a^b f(t) \overline{\phi_n(t)} \, dt \quad (n=1, 2, 3, \dots)
  \end{equation}
  we call $c_n$ the nth Fourier coefficient of f relative to
  $\{\phi_n\}$. We write
  \begin{equation} \label{eq:chap8:gen_fourier_series}
    f(x) \sim \sum_{n=1}^\infty c_n \phi_n(x)
  \end{equation}
  and call this series the Fourier series of f (relative to
  $\{\phi_n\}$). Note that the symbol $\sim$ used in
  \eqref{eq:chap8:gen_fourier_series} implies nothing about the
  convergence of the series; it merely says that the coefficients are
  given by \eqref{eq:chap8:gen_fourier_coeff}.
\end{definition}

% --- End of content chunk ---

% --- Content from Thm 8.11 to Cor 8.14 (Proofs Omitted) to append ---

% The following theorems show that the partial sums of the Fourier series
% of f have a certain minimum property. We shall assume here and in the rest of
% this chapter that f is Riemann-integrable, although this hypothesis
% can be weakened.

\begin{theorem}[Best Mean Square Approximation] % Theorem 8.11 from Ch 8 PDF
  \label{thm:chap8:best_mean_sq_approx}
  Let $\{\phi_n\}$ be orthonormal on $[a, b]$. Let
  \begin{equation} \label{eq:chap8:fourier_partial_sum}
    s_n(x) = \sum_{m=1}^n c_m \phi_m(x)
  \end{equation}
  be the nth partial sum of the Fourier series of f
  (\autoref{eq:chap8:gen_fourier_series}), and suppose
  \begin{equation} \label{eq:chap8:gen_trig_poly}
    t_n(x) = \sum_{m=1}^n \gamma_m \phi_m(x).
  \end{equation}
  Then
  \begin{equation} \label{eq:chap8:min_sq_error}
    \int_a^b \abs{f - s_n}^2 \, dx \le \int_a^b \abs{f - t_n}^2 \, dx,
  \end{equation}
  and equality holds if and only if
  \begin{equation} \label{eq:chap8:min_sq_error_cond}
    \gamma_m = c_m \quad (m=1, \dots, n).
  \end{equation}
  That is to say, among all functions $t_n$, $s_n$ gives the best
  possible mean square approximation to f.
  % Proof Omitted (Expands the integral |f-t_n|^2)
  % Note: Proof leads to the identity:
  % \int_a^b |s_n(x)|^2 dx = sum_{m=1}^n |c_m|^2 <= \int_a^b |f(x)|^2 dx
\end{theorem}

\begin{theorem}[Bessel's Inequality] % Theorem 8.12 from Ch 8 PDF
  \label{thm:chap8:bessels_inequality}
  If $\{\phi_n\}$ is orthonormal on $[a, b]$, and if
  \[ f(x) \sim \sum_{n=1}^\infty c_n \phi_n(x), \]
  then
  \begin{equation} \label{eq:chap8:bessel_inequality}
    \sum_{n=1}^\infty \abs{c_n}^2 \le \int_a^b \abs{f(x)}^2 \, dx.
  \end{equation}
  In particular,
  \begin{equation} \label{eq:chap8:coeffs_to_zero}
    \lim_{n \to \infty} c_n = 0.
  \end{equation}
  % Proof Omitted (Takes limit in identity from Thm 8.11 proof)
\end{theorem}

\begin{remark}[Trigonometric Series Focus] % Remark 8.13 from Ch 8 PDF
  \label{rem:chap8:trig_series_focus}
  From now on we shall deal only with the trigonometric system. We
  shall consider functions f that have period $2\pi$ and that are
  Riemann-integrable on $[-\pi, \pi]$ (and hence on every bounded
  interval). The Fourier series of f is then the series
  \eqref{eq:chap8:trig_series} whose coefficients $c_n$ are given by
  the integrals \eqref{eq:chap8:trig_poly_coeffs}, and
  \begin{equation} \label{eq:chap8:trig_partial_sum_def}
    s_N(x) = s_N(f; x) = \sum_{n=-N}^N c_n e^{inx}
  \end{equation}
  is the Nth partial sum of the Fourier series of f. The inequality
  derived in the proof of \autoref{thm:chap8:best_mean_sq_approx} (or
  \autoref{thm:chap8:bessels_inequality}) now takes the form
  \begin{equation} \label{eq:chap8:trig_partial_sum_norm}
    \frac{1}{2\pi} \int_{-\pi}^{\pi} \abs{s_N(x)}^2 \, dx =
    \sum_{n=-N}^N \abs{c_n}^2 \le \frac{1}{2\pi} \int_{-\pi}^{\pi}
    \abs{f(x)}^2 \, dx = \avgint{\abs{f(x)}^2}.
  \end{equation}
  In order to obtain an expression for $s_N$ that is more manageable
  than \eqref{eq:chap8:trig_partial_sum_def}, we introduce the Dirichlet kernel
  \begin{equation} \label{eq:chap8:dirichlet_kernel}
    D_N(x) = \sum_{n=-N}^N e^{inx} = \frac{\sin((N + 1/2)x)}{\sin(x/2)}.
  \end{equation}
  (The second equality follows from summing the geometric progression.)
  By \eqref{eq:chap8:trig_poly_coeffs} and
  \eqref{eq:chap8:trig_partial_sum_def}, we have
  \begin{align*}
    s_N(f; x) &= \sum_{n=-N}^N \left( \frac{1}{2\pi}
    \int_{-\pi}^{\pi} f(t) e^{-int} \, dt \right) e^{inx} \\
    &= \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) \sum_{n=-N}^N e^{in(x-t)} \, dt,
  \end{align*}
  so that
  \begin{equation} \label{eq:chap8:partial_sum_convolution}
    s_N(f; x) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) D_N(x-t) \, dt
    = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x-t) D_N(t) \, dt.
  \end{equation}
  The periodicity of all functions involved shows that it is
  immaterial over which interval we integrate, as long as its length
  is $2\pi$. This shows that the two integrals in
  \eqref{eq:chap8:partial_sum_convolution} are equal.
\end{remark}

% We shall prove just one theorem about the pointwise convergence of
% Fourier series.

\begin{theorem}[Pointwise Convergence Condition] % Theorem 8.14 from Ch 8 PDF
  \label{thm:chap8:fourier_pointwise_conv}
  If, for some x, there are constants $\delta > 0$ and $M < \infty$ such that
  \begin{equation} \label{eq:chap8:pointwise_holder_cond}
    \abs{f(x+t) - f(x)} \le M \abs{t}
  \end{equation}
  for all $t \in (-\delta, \delta)$, then
  \begin{equation} \label{eq:chap8:pointwise_conv_result}
    \lim_{N \to \infty} s_N(f; x) = f(x).
  \end{equation}
  % Proof Omitted (Uses properties of Dirichlet kernel and
  % Riemann-Lebesgue Lemma implicitly via Eq (74))
\end{theorem}

\begin{corollary}[Localization Theorem] % Corollary to 8.14 from Ch 8 PDF
  \label{cor:chap8:fourier_localization}
  If $f(x) = 0$ for all x in some segment J, then $\lim_{N \to
  \infty} s_N(f; x) = 0$ for every $x \in J$.
  (Formulation: If $f(t) = g(t)$ for all t in some neighborhood of x,
    then $s_N(f; x) - s_N(g; x) = s_N(f-g; x) \to 0$ as $N \to \infty$.
    This shows convergence behavior depends only on local values,
  contrasting with power series \autoref{thm:chap8:ps_uniqueness}).
\end{corollary}

% --- End of content chunk ---

% --- Content from Thm 8.15 to Thm 8.18 (Proofs Omitted) to append ---

% We conclude with two other approximation theorems.

\begin{theorem}[Uniform Approximation by Trig Polynomials] % Theorem
  % 8.15 from Ch 8 PDF
  \label{thm:chap8:trig_poly_approx}
  If f is continuous (with period $2\pi$) and if $\epsilon > 0$, then
  there is a trigonometric polynomial P such that
  \[ \abs{P(x) - f(x)} < \epsilon \]
  for all real x.
  % Proof Omitted (Uses Stone-Weierstrass Thm 7.33 applied to
  % functions on the unit circle)
\end{theorem}

% A more precise form of this theorem appears in Exercise 15.

\begin{theorem}[Parseval's Theorem] % Theorem 8.16 from Ch 8 PDF
  \label{thm:chap8:parsevals_theorem}
  Suppose f and g are Riemann-integrable functions with period $2\pi$, and
  \begin{align}
    f(x) &\sim \sum_{n=-\infty}^\infty c_n e^{inx},
    \label{eq:chap8:f_fourier} \\
    g(x) &\sim \sum_{n=-\infty}^\infty \gamma_n e^{inx}.
    \label{eq:chap8:g_fourier}
  \end{align}
  Then
  \begin{gather}
    \lim_{N \to \infty} \avgint{\abs{f(x) - s_N(f; x)}^2} = 0,
    \label{eq:chap8:mean_sq_conv} \\
    \frac{1}{2\pi} \int_{-\pi}^{\pi} f(x) \overline{g(x)} \, dx =
    \sum_{n=-\infty}^\infty c_n \overline{\gamma_n},
    \label{eq:chap8:parseval_fg} \\
    \frac{1}{2\pi} \int_{-\pi}^{\pi} \abs{f(x)}^2 \, dx =
    \sum_{n=-\infty}^\infty \abs{c_n}^2. \label{eq:chap8:parseval_ff}
  \end{gather}
  (Using `$\avgint{}$` for the normalized integral).
  % Proof Omitted (Uses approximation by continuous function and trig
  % poly, Thm 8.15, Thm 8.11, Schwarz inequality)
\end{theorem}

% A more general version of Theorem 8.16 appears in Chap. 11.

\section{The Gamma Function}
\label{sec:chap8:gamma_function}

% This function is closely related to factorials and crops up in many unexpected
% places in analysis. Its origin, history, and development are very
% well described
% in an interesting article by P. J. Davis (Amer. Math. Monthly, vol. 66, 1959,
% pp. 849-869). Artin's book (cited in the Bibliography) is another good elemen-
% tary introduction.
% Our presentation will be very condensed, with only a few comments after
% each theorem. This section may thus be regarded as a large exercise, and as an
% opportunity to apply some of the material that has been presented so far.

\begin{definition}[Gamma Function] % Definition 8.17 from Ch 8 PDF
  \label{def:chap8:gamma_function}
  For $0 < x < \infty$,
  \begin{equation} \label{eq:chap8:gamma_def}
    \Gamma(x) = \int_0^\infty t^{x-1} e^{-t} \, dt.
  \end{equation}
  The integral converges for these x. (When $x<1$, both 0 and
  $\infty$ have to be looked at.)
\end{definition}

\begin{theorem}[Properties of Gamma Function] % Theorem 8.18 from Ch 8 PDF
  \label{thm:chap8:gamma_properties}
  (a) The functional equation
  \[ \Gamma(x+1) = x \Gamma(x) \]
  holds if $0 < x < \infty$.
  (b) $\Gamma(n+1) = n!$ for $n=1, 2, 3, \dots$.
  (c) $\log \Gamma$ is convex on $(0, \infty)$.
  % Proof Omitted (Uses integration by parts for (a), induction for
  % (b), HÃ¶lder's inequality for (c))
\end{theorem}

% It is a rather surprising fact, discovered by Bohr and Mollerup, that
% these three properties characterize Gamma completely.

% --- End of content chunk ---

% --- Content from Thm 8.19 to Remark 8.22 (Proofs Omitted) to append ---

\begin{theorem}[Bohr-Mollerup Theorem] % Theorem 8.19 from Ch 8 PDF
  \label{thm:chap8:bohr_mollerup}
  If f is a positive function on $(0, \infty)$ such that
  (a) $f(x+1) = x f(x)$,
  (b) $f(1) = 1$,
  (c) $\log f$ is convex,
  then $f(x) = \Gamma(x)$.
  % Proof Omitted (Shows uniqueness based on convexity and recurrence)
\end{theorem}

% As a by-product we obtain the relation
\begin{equation} \label{eq:chap8:gamma_limit_repr}
  \Gamma(x) = \lim_{n \to \infty} \frac{n! n^x}{x(x+1)\dots(x+n)}
\end{equation}
% holds for all x > 0.

\begin{theorem}[Beta Function] % Theorem 8.20 from Ch 8 PDF
  \label{thm:chap8:beta_function}
  If $x>0$ and $y>0$, then
  \begin{equation} \label{eq:chap8:beta_integral}
    \int_0^1 t^{x-1} (1-t)^{y-1} \, dt = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}.
  \end{equation}
  This integral is the so-called beta function $B(x, y)$.
  % Proof Omitted (Uses Thm 8.19 applied to f(x) = Gamma(x+y)/Gamma(y) * B(x,y))
\end{theorem}

\begin{remark}[Consequences of Beta Function / Gamma Values] % Remark
  % 8.21 from Ch 8 PDF
  \label{rem:chap8:gamma_consequences}
  The substitution $t = \sin^2 \theta$ turns \eqref{eq:chap8:beta_integral} into
  \begin{equation} \label{eq:chap8:beta_trig_form}
    2 \int_0^{\pi/2} (\sin \theta)^{2x-1} (\cos \theta)^{2y-1} \,
    d\theta = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}.
  \end{equation}
  The special case $x = y = 1/2$ gives $\int_0^{\pi/2} 2 \, d\theta =
  \pi = \frac{\Gamma(1/2)\Gamma(1/2)}{\Gamma(1)} = (\Gamma(1/2))^2$, hence
  \begin{equation} \label{eq:chap8:gamma_half}
    \Gamma(1/2) = \sqrt{\pi}.
  \end{equation}
  The substitution $t=s^2$ turns the Gamma definition
  \eqref{eq:chap8:gamma_def} into
  \begin{equation} \label{eq:chap8:gamma_gauss_form}
    \Gamma(x) = 2 \int_0^\infty s^{2x-1} e^{-s^2} \, ds \quad (0 < x < \infty).
  \end{equation}
  The special case $x=1/2$ gives $\Gamma(1/2) = 2 \int_0^\infty
  e^{-s^2} \, ds$. Combined with \eqref{eq:chap8:gamma_half}, we get
  the Gaussian integral:
  \begin{equation} \label{eq:chap8:gaussian_integral}
    \int_{-\infty}^\infty e^{-s^2} \, ds = \sqrt{\pi}.
  \end{equation}
  The identity (Duplication Formula)
  \begin{equation} \label{eq:chap8:gamma_duplication}
    \Gamma(x) = \frac{2^{x-1}}{\sqrt{\pi}}
    \Gamma\left(\frac{x}{2}\right) \Gamma\left(\frac{x+1}{2}\right)
  \end{equation}
  follows directly from \autoref{thm:chap8:bohr_mollerup}.
\end{remark}

\begin{remark}[Stirling's Formula] % Remark 8.22 from Ch 8 PDF
  \label{rem:chap8:stirling}
  This provides a simple approximate expression for $\Gamma(x+1)$
  when x is large (hence for n! when n is large). The formula is
  \begin{equation} \label{eq:chap8:stirling_formula}
    \lim_{x \to \infty} \frac{\Gamma(x+1)}{(x/e)^x \sqrt{2\pi x}} = 1.
  \end{equation}
  % Outline of proof method provided in Rudin's text omitted here.
\end{remark}

% --- End of Chapter 8 main content chunk (Proofs Omitted) ---
% --- End of chapters/chapter8.tex ---
